{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/udit/programs/Synaptic-Flow/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/udit/programs/Synaptic-Flow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "\"dataset\" : \"mnist\", # ['mnist','cifar10','cifar100','tiny-imagenet','imagenet']\n",
    "\"model\" : \"fc\", # ['fc','fc-orth','conv','conv-orth','strconv', ... <take rest from main.py>]\n",
    "\"model_class\" : \"default\", # ['default','lottery','tinyimagenet','imagenet']\n",
    "\"dense_classifier\" : False,\n",
    "\"pretrained\" : False,\n",
    "\"optimizer\" : \"adam\", # ['sgd','momentum','adam','rms']\n",
    "\"train_batch_size\" : 128,\n",
    "\"test_batch_size\" : 128,\n",
    "\"pre_epochs\" : 0, # number of epochs to train before pruning\n",
    "\"post_epochs\" : 20, # number of epochs to train after pruning\n",
    "\"lr\" : 0.001,\n",
    "\"lr_drops\" : [30, 60, 80],\n",
    "\"lr_drop_rate\" : 0.1,\n",
    "\"weight_decay\" : 1e-4 ,\n",
    "\"save\" : False,\n",
    "\"scale\" : 1,\n",
    "\n",
    "# pruning args\n",
    "\"pruner\" : \"synflow\", # ['rand','mag','snip','grasp','synflow']\n",
    "\"compression\" : 1.0, # quotient of prunable non-zero prunable parameters before and after pruning (defaul\"t: 1.0)\n",
    "\"prune_epochs\" : 10, # number of iterations for scoring (defaul\"t: 1)\n",
    "\"compression_schedule\" : \"exponential\", # ['linear','exponential']\n",
    "\"mask_scope\" : \"global\", # ['global','local']\n",
    "\"prune_dataset_ratio\" : 10, # ratio of prune dataset size and number of classes (defaul\"t: 10)'\n",
    "\"prune_batch_size\" : 256, # input batch size for pruning (defaul\"t: 256)\n",
    "\"prune_bias\" : False,\n",
    "\"prune_batchnorm\" : False,\n",
    "\"prune_residual\" : False,\n",
    "\"reinitialize\" : False, # whether to reinitialize weight parameters after pruning (defaul\"t: False)\n",
    "\"pruner_list\" : [], # list of pruning strategies for singleshot (defaul\"t: [])\n",
    "\"prune_epoch_list\" : [], # list of prune epochs for singleshot (defaul\"t: [])\n",
    "\"compression_list\" : [], # list of compression ratio exponents for singleshot/multishot (defaul\"t: [])\n",
    "\"level_list\" : [],\n",
    "\"experiment\" : \"prune-only\", # ['example',\"prune-only\",'singleshot','multishot',\n",
    "    # 'unit-conservation', 'layer-conservation','imp-conservation','schedule-conservation']\n",
    "\"expid\" : \"\",\n",
    "\"result_dir\" : \"Results/data\",\n",
    "\"gpu\" : \"0\",\n",
    "\"workers\" : 4,\n",
    "\"no_cuda\" : False,\n",
    "\"seed\" : 1,\n",
    "\"verbose\" : False,\n",
    "\"save_pruned\" : False,\n",
    "\"save_pruned_path\" : \"Results/pruned\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Utils import load\n",
    "from Utils import generator\n",
    "from Utils import metrics\n",
    "from prune import *\n",
    "from train import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "def prune_only(args):\n",
    "    print(\"Pruning only\")\n",
    "    \n",
    "    ## Random Seed and Device ##\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    device = load.device(args[\"gpu\"])\n",
    "    shuffle = False\n",
    "    ## Data ##\n",
    "    print('Loading {} dataset.'.format(args[\"dataset\"]))\n",
    "    input_shape, num_classes = load.dimension(args[\"dataset\"]) \n",
    "    prune_loader = load.dataloader(args[\"dataset\"], args[\"prune_batch_size\"], True, args[\"workers\"]) #, \n",
    "                                   # args[\"prune_dataset_ratio\"] * num_classes)\n",
    "    \n",
    "    train_loader = load.dataloader(args[\"dataset\"], args[\"train_batch_size\"], False, args[\"workers\"])\n",
    "    test_loader = load.dataloader(args[\"dataset\"], args[\"test_batch_size\"], False, args[\"workers\"])\n",
    "    \n",
    "    \n",
    "    ## Model, Loss, Optimizer ##\n",
    "    print('Creating {}-{} model.'.format(args[\"model_class\"], args[\"model\"]))        \n",
    "\n",
    "    model = load.model(args[\"model\"], args[\"model_class\"])(input_shape, \n",
    "                                                     num_classes, \n",
    "                                                     args[\"dense_classifier\"], \n",
    "                                                     args[\"pretrained\"]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "        \n",
    "    opt_class, opt_kwargs = load.optimizer(args[\"optimizer\"])\n",
    "    optimizer = opt_class(generator.parameters(model), lr=args[\"lr\"], \n",
    "                          weight_decay=args[\"weight_decay\"], **opt_kwargs)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args[\"lr_drops\"], \n",
    "                                                     gamma=args[\"lr_drop_rate\"])\n",
    "\n",
    "    \n",
    "    #print(f\"Kernel: {kernel}\")\n",
    "    \n",
    "    ## Pre-Train ##\n",
    "    #print('Pre-Train for {} epochs.'.format(args.pre_epochs))\n",
    "    pre_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                 test_loader, device, 0, args[\"verbose\"])\n",
    "    \n",
    "    \n",
    "    ## Prune ##\n",
    "    print('Pruning with {} for {} epochs.'.format(args[\"pruner\"], args[\"prune_epochs\"]))\n",
    "    pruner = load.pruner(args[\"pruner\"])(generator.masked_parameters(model, args[\"prune_bias\"], \n",
    "                       args[\"prune_batchnorm\"], args[\"prune_residual\"]))\n",
    "    sparsity = 10**(-float(args[\"compression\"]))\n",
    "    print(\"Sparsity: {}\".format(sparsity))\n",
    "    save_pruned_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "    if (args[\"save_pruned\"]):\n",
    "        print(\"Saving pruned models to: %s\" % (save_pruned_path, ))\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_pruned_path)\n",
    "    prune_loop(model, loss, pruner, prune_loader, device, sparsity, \n",
    "               args[\"compression_schedule\"], args[\"mask_scope\"], args[\"prune_epochs\"], \n",
    "               args[\"reinitialize\"], args[\"save_pruned\"], save_pruned_path)\n",
    "    \n",
    "    prune_result = metrics.summary(model, pruner.scores,\n",
    "                                   metrics.flop(model, input_shape, device),\n",
    "                                   lambda p: generator.prunable(p, args[\"prune_batchnorm\"], \n",
    "                                    args[\"prune_residual\"]))\n",
    "    total_params = int((prune_result['sparsity'] * prune_result['size']).sum())\n",
    "    possible_params = prune_result['size'].sum()\n",
    "    total_flops = int((prune_result['sparsity'] * prune_result['flops']).sum())\n",
    "    possible_flops = prune_result['flops'].sum()\n",
    "    print(\"Parameter Sparsity: {}/{} ({:.4f})\".format(total_params, \n",
    "                                                      possible_params, total_params / possible_params))\n",
    "    print(\"FLOP Sparsity: {}/{} ({:.4f})\".format(total_flops, \n",
    "                                                 possible_flops, total_flops / possible_flops))\n",
    "    \n",
    "    ## Post-Train ##\n",
    "    #print('Post-Training for {} epochs.'.format(args.post_epochs))\n",
    "    post_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                  test_loader, device, args[\"post_epochs\"], args[\"verbose\"])\n",
    "    \n",
    "    print(save_pruned_path)\n",
    "    ## Display Results ##\n",
    "#     frames = [pre_result.head(1), post_result.head(1), post_result.tail(1)]\n",
    "#     train_result = pd.concat(frames, keys=['Init.', 'Post-Prune', \"Final\"])\n",
    "\n",
    "#     print(\"Train results:\\n\", train_result)\n",
    "#     print(\"Prune results:\\n\", prune_result)\n",
    "    if (args[\"save_result\"]):\n",
    "        save_result_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_result_path)\n",
    "        post_result.to_csv(save_result_path + \"/%s\" % (args[\"dataset\"] + \"_\" + str(args[\"seed\"]) \n",
    "                                                                        + \"_\" + str(args[\"compression\"]) + \".csv\"))\n",
    "    ## Save Results and Model ##\n",
    "    if args[\"save\"]:\n",
    "        print('Saving results.')\n",
    "        pre_result.to_pickle(\"{}/pre-train.pkl\".format(args[\"result_dir\"]))\n",
    "        post_result.to_pickle(\"{}/post-train.pkl\".format(args[\"result_dir\"]))\n",
    "        prune_result.to_pickle(\"{}/compression.pkl\".format(args[\"result_dir\"]))\n",
    "        torch.save(model.state_dict(),\"{}/model.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(optimizer.state_dict(),\"{}/optimizer.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(pruner.state_dict(),\"{}/pruner.pt\".format(args[\"result_dir\"]))\n",
    "    \n",
    "    # to change\n",
    "    return post_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar10 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11-bn model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow for 100 epochs.\n",
      "Sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:18<00:04,  4.21it/s]Traceback (most recent call last):\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 930550/9231114 (0.1008)\n",
      "FLOP Sparsity: 41381662/153224202 (0.2701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [02:02<00:00,  4.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.311064</td>\n",
       "      <td>10.15</td>\n",
       "      <td>48.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.570313</td>\n",
       "      <td>1.213881</td>\n",
       "      <td>57.71</td>\n",
       "      <td>96.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.064090</td>\n",
       "      <td>0.869591</td>\n",
       "      <td>69.33</td>\n",
       "      <td>98.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680498</td>\n",
       "      <td>0.632139</td>\n",
       "      <td>78.12</td>\n",
       "      <td>99.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.593127</td>\n",
       "      <td>78.91</td>\n",
       "      <td>99.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.193451</td>\n",
       "      <td>1.273772</td>\n",
       "      <td>61.58</td>\n",
       "      <td>96.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.167934</td>\n",
       "      <td>0.436271</td>\n",
       "      <td>84.38</td>\n",
       "      <td>99.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.129766</td>\n",
       "      <td>0.109078</td>\n",
       "      <td>97.07</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.057542</td>\n",
       "      <td>0.081676</td>\n",
       "      <td>97.90</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026889</td>\n",
       "      <td>0.044886</td>\n",
       "      <td>98.91</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>99.97</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   2.311064          10.15          48.38\n",
       "1     1.570313   1.213881          57.71          96.03\n",
       "2     1.064090   0.869591          69.33          98.54\n",
       "3     0.680498   0.632139          78.12          99.10\n",
       "4     0.345361   0.593127          78.91          99.28\n",
       "5     0.193451   1.273772          61.58          96.73\n",
       "6     0.167934   0.436271          84.38          99.94\n",
       "7     0.129766   0.109078          97.07         100.00\n",
       "8     0.057542   0.081676          97.90         100.00\n",
       "9     0.026889   0.044886          98.91         100.00\n",
       "10    0.010404   0.006221          99.97         100.00\n",
       "11    0.003614   0.003342         100.00         100.00\n",
       "12    0.001701   0.002341         100.00         100.00\n",
       "13    0.001298   0.002007         100.00         100.00\n",
       "14    0.001110   0.001807         100.00         100.00\n",
       "15    0.000987   0.001669         100.00         100.00\n",
       "16    0.000897   0.001572         100.00         100.00\n",
       "17    0.000829   0.001501         100.00         100.00\n",
       "18    0.000775   0.001447         100.00         100.00\n",
       "19    0.000730   0.001407         100.00         100.00\n",
       "20    0.000691   0.001377         100.00         100.00\n",
       "21    0.000655   0.001350         100.00         100.00\n",
       "22    0.000620   0.001326         100.00         100.00\n",
       "23    0.000586   0.001303         100.00         100.00\n",
       "24    0.000552   0.001286         100.00         100.00\n",
       "25    0.000518   0.001270         100.00         100.00\n",
       "26    0.000484   0.001262         100.00         100.00\n",
       "27    0.000451   0.001257         100.00         100.00\n",
       "28    0.000420   0.001262         100.00         100.00\n",
       "29    0.000394   0.001283         100.00         100.00\n",
       "30    0.000372   0.001346         100.00         100.00"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 30\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11-bn\"\n",
    "args['dataset'] =  'cifar10' # 'tiny-imagenet'\n",
    "args['lr'] = 0.0001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow\"\n",
    "args['compression'] = 1.0\n",
    "args['verbosse'] = True\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar100 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11-bn model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow-dist for 100 epochs.\n",
      "Sparsity: 0.01\n",
      "Feeding image_mean\n",
      "Calculating mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 101045/9277284 (0.0109)\n",
      "FLOP Sparsity: 7096682/153270372 (0.0463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [02:02<00:00,  4.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.605134</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.122572</td>\n",
       "      <td>4.133383</td>\n",
       "      <td>7.90</td>\n",
       "      <td>28.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.627807</td>\n",
       "      <td>3.659594</td>\n",
       "      <td>12.89</td>\n",
       "      <td>38.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.325873</td>\n",
       "      <td>3.256981</td>\n",
       "      <td>20.35</td>\n",
       "      <td>48.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.073351</td>\n",
       "      <td>3.174447</td>\n",
       "      <td>21.60</td>\n",
       "      <td>50.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.851671</td>\n",
       "      <td>3.011569</td>\n",
       "      <td>24.31</td>\n",
       "      <td>53.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.667793</td>\n",
       "      <td>2.935781</td>\n",
       "      <td>26.18</td>\n",
       "      <td>56.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.457366</td>\n",
       "      <td>3.007600</td>\n",
       "      <td>25.97</td>\n",
       "      <td>56.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.280894</td>\n",
       "      <td>2.480884</td>\n",
       "      <td>35.26</td>\n",
       "      <td>67.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.116923</td>\n",
       "      <td>2.480918</td>\n",
       "      <td>34.45</td>\n",
       "      <td>67.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.966676</td>\n",
       "      <td>2.278601</td>\n",
       "      <td>38.53</td>\n",
       "      <td>72.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.806086</td>\n",
       "      <td>2.189503</td>\n",
       "      <td>40.39</td>\n",
       "      <td>73.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.653870</td>\n",
       "      <td>2.175944</td>\n",
       "      <td>41.48</td>\n",
       "      <td>74.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.515062</td>\n",
       "      <td>2.173573</td>\n",
       "      <td>41.80</td>\n",
       "      <td>75.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.364565</td>\n",
       "      <td>1.832407</td>\n",
       "      <td>49.24</td>\n",
       "      <td>81.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.236348</td>\n",
       "      <td>1.705384</td>\n",
       "      <td>52.81</td>\n",
       "      <td>83.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.122899</td>\n",
       "      <td>1.709127</td>\n",
       "      <td>51.64</td>\n",
       "      <td>84.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.994578</td>\n",
       "      <td>1.530802</td>\n",
       "      <td>56.04</td>\n",
       "      <td>86.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.898504</td>\n",
       "      <td>1.415637</td>\n",
       "      <td>59.62</td>\n",
       "      <td>88.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.822735</td>\n",
       "      <td>1.460802</td>\n",
       "      <td>57.65</td>\n",
       "      <td>88.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.747731</td>\n",
       "      <td>1.275635</td>\n",
       "      <td>62.26</td>\n",
       "      <td>90.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.672382</td>\n",
       "      <td>1.274929</td>\n",
       "      <td>62.27</td>\n",
       "      <td>90.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.618825</td>\n",
       "      <td>1.274265</td>\n",
       "      <td>62.79</td>\n",
       "      <td>91.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.571365</td>\n",
       "      <td>1.218266</td>\n",
       "      <td>64.35</td>\n",
       "      <td>92.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.546672</td>\n",
       "      <td>1.084295</td>\n",
       "      <td>67.25</td>\n",
       "      <td>94.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.525665</td>\n",
       "      <td>1.077180</td>\n",
       "      <td>67.32</td>\n",
       "      <td>94.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.489932</td>\n",
       "      <td>0.842158</td>\n",
       "      <td>74.22</td>\n",
       "      <td>96.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.425918</td>\n",
       "      <td>1.000464</td>\n",
       "      <td>69.69</td>\n",
       "      <td>94.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.403638</td>\n",
       "      <td>1.065403</td>\n",
       "      <td>68.06</td>\n",
       "      <td>94.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.397767</td>\n",
       "      <td>1.034864</td>\n",
       "      <td>68.82</td>\n",
       "      <td>94.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.351385</td>\n",
       "      <td>0.979985</td>\n",
       "      <td>70.70</td>\n",
       "      <td>94.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   4.605134           1.02           5.48\n",
       "1     4.122572   4.133383           7.90          28.18\n",
       "2     3.627807   3.659594          12.89          38.51\n",
       "3     3.325873   3.256981          20.35          48.02\n",
       "4     3.073351   3.174447          21.60          50.37\n",
       "5     2.851671   3.011569          24.31          53.32\n",
       "6     2.667793   2.935781          26.18          56.45\n",
       "7     2.457366   3.007600          25.97          56.49\n",
       "8     2.280894   2.480884          35.26          67.60\n",
       "9     2.116923   2.480918          34.45          67.71\n",
       "10    1.966676   2.278601          38.53          72.30\n",
       "11    1.806086   2.189503          40.39          73.61\n",
       "12    1.653870   2.175944          41.48          74.60\n",
       "13    1.515062   2.173573          41.80          75.99\n",
       "14    1.364565   1.832407          49.24          81.61\n",
       "15    1.236348   1.705384          52.81          83.18\n",
       "16    1.122899   1.709127          51.64          84.31\n",
       "17    0.994578   1.530802          56.04          86.61\n",
       "18    0.898504   1.415637          59.62          88.27\n",
       "19    0.822735   1.460802          57.65          88.27\n",
       "20    0.747731   1.275635          62.26          90.89\n",
       "21    0.672382   1.274929          62.27          90.92\n",
       "22    0.618825   1.274265          62.79          91.05\n",
       "23    0.571365   1.218266          64.35          92.18\n",
       "24    0.546672   1.084295          67.25          94.25\n",
       "25    0.525665   1.077180          67.32          94.08\n",
       "26    0.489932   0.842158          74.22          96.31\n",
       "27    0.425918   1.000464          69.69          94.87\n",
       "28    0.403638   1.065403          68.06          94.79\n",
       "29    0.397767   1.034864          68.82          94.51\n",
       "30    0.351385   0.979985          70.70          94.97"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 30\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11-bn\"\n",
    "args['dataset'] =  'cifar100' # 'tiny-imagenet'\n",
    "args['lr'] = 0.001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow-dist\"\n",
    "args['compression'] = 2.0\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading mnist dataset.\n",
      "Creating default-fc model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow-dist-l2 for 1 epochs.\n",
      "Sparsity: 0.1\n",
      "Feeding image_mean\n",
      "Calculating mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 12449/119910 (0.1038)\n",
      "FLOP Sparsity: 12449/119910 (0.1038)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results/pruned/default/fc/synflow-dist-l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.304020</td>\n",
       "      <td>10.28</td>\n",
       "      <td>51.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.302430</td>\n",
       "      <td>2.301438</td>\n",
       "      <td>11.35</td>\n",
       "      <td>51.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.301986</td>\n",
       "      <td>2.301421</td>\n",
       "      <td>11.35</td>\n",
       "      <td>51.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0         NaN   2.304020          10.28          51.34\n",
       "1    2.302430   2.301438          11.35          51.11\n",
       "2    2.301986   2.301421          11.35          51.11"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 2\n",
    "args['prune_epochs'] = 1\n",
    "args['model'] = \"fc\"\n",
    "args['dataset'] =  'mnist'\n",
    "args['lr'] = 0.01\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'default'\n",
    "args['pruner'] = \"synflow-dist-l2\"\n",
    "args['compression'] = 1.0\n",
    "args['save_result'] = True\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG11: \n",
    "- CIFAR 10 - adam, 0.001, 60/120, 0.1, 1e-4 both compression: 1 and 2, 100/160\n",
    "- CIFAR 100 - adam/0.0005, mom/0.001, 60/120, 0.1, 1e-4 both compression: 1 and 2, 100/160\n",
    "- TINYIMAGENET - \n",
    "\n",
    "VGG11-BN:\n",
    "- CIFAR 10 - adam, 0.0001 (comp: 1), 0.0001 (comp: 2), 60/120, 0.1, 1e-4 100/160\n",
    "- CIFAR 100 - adam, 0.0001 (comp: 1), 0.0005 (comp: 2), 60/120, 0.1, 1e-4 100/160\n",
    "\n",
    "VGG16:\n",
    "- CIFAR 10 - adam, 0.001, 60/120, 0.1, 1e-4, compression 1\n",
    "- CIFAR 100 - adam, 0.0001, 60/120, 0.1, 1e-4, compression 1\n",
    "\n",
    "VGG16-BN:\n",
    "- CIFAR 10 - adam, 0.001 1/2 60/120, 0.1, 1e-4\n",
    "- CIFAR 100 - adam, 0.001 (comp: 1)/ 0.0001 (comp: 2), 60/120, 0.1, 1e-4\n",
    "\n",
    "Resnet20:\n",
    "- CIFAR10 - adam 0.0005 (comp: 1), 0.0005 (comp: 2) \n",
    "- CIFAR100 - adam 0.01\n",
    "\n",
    "Resnet18:\n",
    "- Tiny Imagenet:\n",
    "\n",
    "WideResnet20:\n",
    "- CIFAR10 - adam 0.001 (comp: 1/2),\n",
    "- CIFAR100 - adam 0.001 (comp: 1/2), \n",
    "\n",
    "WideResnet18:\n",
    "- Tiny Imagenet: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
