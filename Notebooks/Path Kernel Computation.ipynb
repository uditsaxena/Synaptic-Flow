{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/udit/programs/Synaptic-Flow/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/udit/programs/Synaptic-Flow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "\"dataset\" : \"mnist\", # ['mnist','cifar10','cifar100','tiny-imagenet','imagenet']\n",
    "\"model\" : \"fc\", # ['fc','fc-orth','conv','conv-orth','strconv', ... <take rest from main.py>]\n",
    "\"model_class\" : \"default\", # ['default','lottery','tinyimagenet','imagenet']\n",
    "\"dense_classifier\" : False,\n",
    "\"pretrained\" : False,\n",
    "\"optimizer\" : \"adam\", # ['sgd','momentum','adam','rms']\n",
    "\"train_batch_size\" : 128,\n",
    "\"test_batch_size\" : 128,\n",
    "\"pre_epochs\" : 0, # number of epochs to train before pruning\n",
    "\"post_epochs\" : 20, # number of epochs to train after pruning\n",
    "\"lr\" : 0.001,\n",
    "\"lr_drops\" : [30, 60, 80],\n",
    "\"lr_drop_rate\" : 0.1,\n",
    "\"weight_decay\" : 1e-4 ,\n",
    "\"save\" : False,\n",
    "\"scale\" : 1,\n",
    "\n",
    "# pruning args\n",
    "\"pruner\" : \"synflow\", # ['rand','mag','snip','grasp','synflow']\n",
    "\"compression\" : 1.0, # quotient of prunable non-zero prunable parameters before and after pruning (defaul\"t: 1.0)\n",
    "\"prune_epochs\" : 10, # number of iterations for scoring (defaul\"t: 1)\n",
    "\"compression_schedule\" : \"exponential\", # ['linear','exponential']\n",
    "\"mask_scope\" : \"global\", # ['global','local']\n",
    "\"prune_dataset_ratio\" : 10, # ratio of prune dataset size and number of classes (defaul\"t: 10)'\n",
    "\"prune_batch_size\" : 256, # input batch size for pruning (defaul\"t: 256)\n",
    "\"prune_bias\" : False,\n",
    "\"prune_batchnorm\" : False,\n",
    "\"prune_residual\" : False,\n",
    "\"reinitialize\" : False, # whether to reinitialize weight parameters after pruning (defaul\"t: False)\n",
    "\"pruner_list\" : [], # list of pruning strategies for singleshot (defaul\"t: [])\n",
    "\"prune_epoch_list\" : [], # list of prune epochs for singleshot (defaul\"t: [])\n",
    "\"compression_list\" : [], # list of compression ratio exponents for singleshot/multishot (defaul\"t: [])\n",
    "\"level_list\" : [],\n",
    "\"experiment\" : \"prune-only\", # ['example',\"prune-only\",'singleshot','multishot',\n",
    "    # 'unit-conservation', 'layer-conservation','imp-conservation','schedule-conservation']\n",
    "\"expid\" : \"\",\n",
    "\"result_dir\" : \"Results/data\",\n",
    "\"gpu\" : \"0\",\n",
    "\"workers\" : 4,\n",
    "\"no_cuda\" : False,\n",
    "\"seed\" : 1,\n",
    "\"verbose\" : False,\n",
    "\"save_pruned\" : False,\n",
    "\"save_pruned_path\" : \"Results/pruned\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Utils import load\n",
    "from Utils import generator\n",
    "from Utils import metrics\n",
    "from prune import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "def prune_only(args):\n",
    "    print(\"Pruning only\")\n",
    "    \n",
    "    ## Random Seed and Device ##\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    device = load.device(args[\"gpu\"])\n",
    "    shuffle = False\n",
    "    ## Data ##\n",
    "    print('Loading {} dataset.'.format(args[\"dataset\"]))\n",
    "    input_shape, num_classes = load.dimension(args[\"dataset\"]) \n",
    "    prune_loader = load.dataloader(args[\"dataset\"], args[\"prune_batch_size\"], True, args[\"workers\"]) #, \n",
    "                                   # args[\"prune_dataset_ratio\"] * num_classes)\n",
    "    \n",
    "    train_loader = load.dataloader(args[\"dataset\"], args[\"train_batch_size\"], False, args[\"workers\"])\n",
    "    test_loader = load.dataloader(args[\"dataset\"], args[\"test_batch_size\"], False, args[\"workers\"])\n",
    "    \n",
    "    \n",
    "    ## Model, Loss, Optimizer ##\n",
    "    print('Creating {}-{} model.'.format(args[\"model_class\"], args[\"model\"]))        \n",
    "\n",
    "    model = load.model(args[\"model\"], args[\"model_class\"])(input_shape, \n",
    "                                                     num_classes, \n",
    "                                                     args[\"dense_classifier\"], \n",
    "                                                     args[\"pretrained\"]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "        \n",
    "    opt_class, opt_kwargs = load.optimizer(args[\"optimizer\"])\n",
    "    optimizer = opt_class(generator.parameters(model), lr=args[\"lr\"], \n",
    "                          weight_decay=args[\"weight_decay\"], **opt_kwargs)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args[\"lr_drops\"], \n",
    "                                                     gamma=args[\"lr_drop_rate\"])\n",
    "\n",
    "    \n",
    "    #print(f\"Kernel: {kernel}\")\n",
    "    \n",
    "    ## Pre-Train ##\n",
    "    #print('Pre-Train for {} epochs.'.format(args.pre_epochs))\n",
    "    pre_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                 test_loader, device, 0, args[\"verbose\"])\n",
    "    \n",
    "    \n",
    "    ## Prune ##\n",
    "    print('Pruning with {} for {} epochs.'.format(args[\"pruner\"], args[\"prune_epochs\"]))\n",
    "    pruner = load.pruner(args[\"pruner\"])(generator.masked_parameters(model, args[\"prune_bias\"], \n",
    "                       args[\"prune_batchnorm\"], args[\"prune_residual\"]))\n",
    "    sparsity = 10**(-float(args[\"compression\"]))\n",
    "    print(\"Sparsity: {}\".format(sparsity))\n",
    "    save_pruned_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "    if (args[\"save_pruned\"]):\n",
    "        print(\"Saving pruned models to: %s\" % (save_pruned_path, ))\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_pruned_path)\n",
    "    prune_loop(model, loss, pruner, prune_loader, device, sparsity, \n",
    "               args[\"compression_schedule\"], args[\"mask_scope\"], args[\"prune_epochs\"], \n",
    "               args[\"reinitialize\"], args[\"save_pruned\"], save_pruned_path)\n",
    "    \n",
    "    prune_result = metrics.summary(model, pruner.scores,\n",
    "                                   metrics.flop(model, input_shape, device),\n",
    "                                   lambda p: generator.prunable(p, args[\"prune_batchnorm\"], \n",
    "                                    args[\"prune_residual\"]))\n",
    "    total_params = int((prune_result['sparsity'] * prune_result['size']).sum())\n",
    "    possible_params = prune_result['size'].sum()\n",
    "    total_flops = int((prune_result['sparsity'] * prune_result['flops']).sum())\n",
    "    possible_flops = prune_result['flops'].sum()\n",
    "    print(\"Parameter Sparsity: {}/{} ({:.4f})\".format(total_params, \n",
    "                                                      possible_params, total_params / possible_params))\n",
    "    print(\"FLOP Sparsity: {}/{} ({:.4f})\".format(total_flops, \n",
    "                                                 possible_flops, total_flops / possible_flops))\n",
    "    \n",
    "    ## Post-Train ##\n",
    "    #print('Post-Training for {} epochs.'.format(args.post_epochs))\n",
    "    post_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                  test_loader, device, args[\"post_epochs\"], args[\"verbose\"])\n",
    "    \n",
    "    print(save_pruned_path)\n",
    "    ## Display Results ##\n",
    "#     frames = [pre_result.head(1), post_result.head(1), post_result.tail(1)]\n",
    "#     train_result = pd.concat(frames, keys=['Init.', 'Post-Prune', \"Final\"])\n",
    "\n",
    "#     print(\"Train results:\\n\", train_result)\n",
    "#     print(\"Prune results:\\n\", prune_result)\n",
    "    if (args[\"save_result\"]):\n",
    "        save_result_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_result_path)\n",
    "        post_result.to_csv(save_result_path + \"/%s\" % (args[\"dataset\"] + \"_\" + str(args[\"seed\"]) \n",
    "                                                                        + \"_\" + str(args[\"compression\"]) + \".csv\"))\n",
    "    ## Save Results and Model ##\n",
    "    if args[\"save\"]:\n",
    "        print('Saving results.')\n",
    "        pre_result.to_pickle(\"{}/pre-train.pkl\".format(args[\"result_dir\"]))\n",
    "        post_result.to_pickle(\"{}/post-train.pkl\".format(args[\"result_dir\"]))\n",
    "        prune_result.to_pickle(\"{}/compression.pkl\".format(args[\"result_dir\"]))\n",
    "        torch.save(model.state_dict(),\"{}/model.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(optimizer.state_dict(),\"{}/optimizer.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(pruner.state_dict(),\"{}/pruner.pt\".format(args[\"result_dir\"]))\n",
    "    \n",
    "    # to change\n",
    "    return post_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loss, optimizer, dataloader, device, epoch, verbose, log_interval=10):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        model_hook(model, output, data, device)\n",
    "        train_loss = loss(output, target)\n",
    "        total += train_loss.item() * data.size(0)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose & (batch_idx % log_interval == 0):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), train_loss.item()))\n",
    "        break\n",
    "    return total / len(dataloader.dataset)\n",
    "\n",
    "def eval(model, loss, dataloader, device, verbose):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct1 = 0\n",
    "    correct5 = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total += loss(output, target).item() * data.size(0)\n",
    "            _, pred = output.topk(5, dim=1)\n",
    "            correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
    "            correct1 += correct[:,:1].sum().item()\n",
    "            correct5 += correct[:,:5].sum().item()\n",
    "    average_loss = total / len(dataloader.dataset)\n",
    "    accuracy1 = 100. * correct1 / len(dataloader.dataset)\n",
    "    accuracy5 = 100. * correct5 / len(dataloader.dataset)\n",
    "    if verbose:\n",
    "        print('Evaluation: Average loss: {:.4f}, Top 1 Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "            average_loss, correct1, len(dataloader.dataset), accuracy1))\n",
    "    return average_loss, accuracy1, accuracy5\n",
    "\n",
    "def train_eval_loop(model, loss, optimizer, scheduler, train_loader, test_loader, device, epochs, verbose):\n",
    "    print(\"HELLO\")\n",
    "    test_loss, accuracy1, accuracy5 = eval(model, loss, test_loader, device, verbose)\n",
    "    rows = [[np.nan, test_loss, accuracy1, accuracy5]]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train(model, loss, optimizer, train_loader, device, epoch, verbose)\n",
    "        test_loss, accuracy1, accuracy5 = eval(model, loss, test_loader, device, verbose)\n",
    "        row = [train_loss, test_loss, accuracy1, accuracy5]\n",
    "        scheduler.step()\n",
    "        rows.append(row)\n",
    "    columns = ['train_loss', 'test_loss', 'top1_accuracy', 'top5_accuracy']\n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def model_hook(model, output, data, device):\n",
    "    squared_copy = copy.deepcopy(model)\n",
    "    \n",
    "    for name, p in squared_copy.named_parameters():\n",
    "        # print(p.data[p.data < 0])\n",
    "        p.data = p.data ** 2\n",
    "    \n",
    "    input_dim = list(data.shape)[1:]\n",
    "    # print(input_dim) # 1, 28, 28\n",
    "    r_pk_ones = torch.ones([1] + input_dim).to(device)\n",
    "    # print(r_pk_ones.shape) # 1, 1, 28, 28\n",
    "    squared_copy(r_pk_ones).sum().backward()\n",
    "    \n",
    "    grad_sq_sum = 0\n",
    "    for name, p in squared_copy.named_parameters():\n",
    "        curr_grad_sq_sum = torch.sum(torch.clone(p.grad).detach())\n",
    "        print(name, curr_grad_sq_sum)\n",
    "        print(\"--\"*5)\n",
    "        grad_sq_sum += curr_grad_sq_sum\n",
    "    print(f\"Squared copy model grad sum: {grad_sq_sum}\")\n",
    "    \n",
    "#     print(\"--\"*20)\n",
    "#     unsq_sum = 0\n",
    "#     for _name, _p in model.named_parameters():\n",
    "#         # print(_name)        \n",
    "#         for name, p in unsquared_copy.named_parameters():\n",
    "#             if _name == name:\n",
    "#                 print(f\"Processing {name}\")\n",
    "#                 p.data = torch.ones(p.data.shape).to(device)\n",
    "#             else:\n",
    "#                 p.data = p.data ** 2\n",
    "        \n",
    "#         unsq_grad_sum = unsquared_copy(r_pk_ones)\n",
    "#         curr_sum = unsq_grad_sum.sum()\n",
    "#         print(curr_sum)\n",
    "#         unsq_sum += curr_sum\n",
    "        \n",
    "#         print(\"--\"*5)\n",
    "#         unsquared_copy = copy.deepcopy(model)        \n",
    "\n",
    "#     print(f\"Unsquared copy sum: {unsq_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar10 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-wide-resnet20 model.\n",
      "HELLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow for 1 epochs.\n",
      "Sparsity: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.40it/s]\n",
      "/home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py:195: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 1085066/1085066 (1.0000)\n",
      "FLOP Sparsity: 163317002/163317002 (1.0000)\n",
      "HELLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.weight tensor(0.1354, device='cuda:0')\n",
      "----------\n",
      "bn.weight tensor(0.0988, device='cuda:0')\n",
      "----------\n",
      "bn.bias tensor(0.2838, device='cuda:0')\n",
      "----------\n",
      "blocks.0.conv1.weight tensor(0.0021, device='cuda:0')\n",
      "----------\n",
      "blocks.0.bn1.weight tensor(1.0625e-05, device='cuda:0')\n",
      "----------\n",
      "blocks.0.bn1.bias tensor(5.7936e-05, device='cuda:0')\n",
      "----------\n",
      "blocks.0.conv2.weight tensor(0.3324, device='cuda:0')\n",
      "----------\n",
      "blocks.0.bn2.weight tensor(0.1330, device='cuda:0')\n",
      "----------\n",
      "blocks.0.bn2.bias tensor(0.2838, device='cuda:0')\n",
      "----------\n",
      "blocks.0.shortcut.weight tensor(0.0987, device='cuda:0')\n",
      "----------\n",
      "blocks.1.conv1.weight tensor(-1.1331, device='cuda:0')\n",
      "----------\n",
      "blocks.1.bn1.weight tensor(-8.3679e-07, device='cuda:0')\n",
      "----------\n",
      "blocks.1.bn1.bias tensor(0.1420, device='cuda:0')\n",
      "----------\n",
      "blocks.1.conv2.weight tensor(1.1911, device='cuda:0')\n",
      "----------\n",
      "blocks.1.bn2.weight tensor(-0.0379, device='cuda:0')\n",
      "----------\n",
      "blocks.1.bn2.bias tensor(0.2333, device='cuda:0')\n",
      "----------\n",
      "blocks.1.shortcut.weight tensor(0.2318, device='cuda:0')\n",
      "----------\n",
      "blocks.2.conv1.weight tensor(-0.3257, device='cuda:0')\n",
      "----------\n",
      "blocks.2.bn1.weight tensor(-2.4601e-06, device='cuda:0')\n",
      "----------\n",
      "blocks.2.bn1.bias tensor(0.2504, device='cuda:0')\n",
      "----------\n",
      "blocks.2.conv2.weight tensor(1.4241, device='cuda:0')\n",
      "----------\n",
      "blocks.2.bn2.weight tensor(-0.1938, device='cuda:0')\n",
      "----------\n",
      "blocks.2.bn2.bias tensor(0.1328, device='cuda:0')\n",
      "----------\n",
      "blocks.2.shortcut.weight tensor(0.1938, device='cuda:0')\n",
      "----------\n",
      "blocks.3.conv1.weight tensor(-0.1196, device='cuda:0')\n",
      "----------\n",
      "blocks.3.bn1.weight tensor(1.4118e-06, device='cuda:0')\n",
      "----------\n",
      "blocks.3.bn1.bias tensor(0.4087, device='cuda:0')\n",
      "----------\n",
      "blocks.3.conv2.weight tensor(-0.7733, device='cuda:0')\n",
      "----------\n",
      "blocks.3.bn2.weight tensor(0.1270, device='cuda:0')\n",
      "----------\n",
      "blocks.3.bn2.bias tensor(0.5665, device='cuda:0')\n",
      "----------\n",
      "blocks.3.shortcut.0.weight tensor(-0.0010, device='cuda:0')\n",
      "----------\n",
      "blocks.3.shortcut.1.weight tensor(0.5430, device='cuda:0')\n",
      "----------\n",
      "blocks.3.shortcut.1.bias tensor(0.5665, device='cuda:0')\n",
      "----------\n",
      "blocks.4.conv1.weight tensor(-2.0394, device='cuda:0')\n",
      "----------\n",
      "blocks.4.bn1.weight tensor(-2.0588e-06, device='cuda:0')\n",
      "----------\n",
      "blocks.4.bn1.bias tensor(0.3930, device='cuda:0')\n",
      "----------\n",
      "blocks.4.conv2.weight tensor(-0.0692, device='cuda:0')\n",
      "----------\n",
      "blocks.4.bn2.weight tensor(-0.2850, device='cuda:0')\n",
      "----------\n",
      "blocks.4.bn2.bias tensor(0.4019, device='cuda:0')\n",
      "----------\n",
      "blocks.4.shortcut.weight tensor(0.6700, device='cuda:0')\n",
      "----------\n",
      "blocks.5.conv1.weight tensor(0.6138, device='cuda:0')\n",
      "----------\n",
      "blocks.5.bn1.weight tensor(-2.4275e-06, device='cuda:0')\n",
      "----------\n",
      "blocks.5.bn1.bias tensor(0.3151, device='cuda:0')\n",
      "----------\n",
      "blocks.5.conv2.weight tensor(0.3691, device='cuda:0')\n",
      "----------\n",
      "blocks.5.bn2.weight tensor(-0.3850, device='cuda:0')\n",
      "----------\n",
      "blocks.5.bn2.bias tensor(0.3035, device='cuda:0')\n",
      "----------\n",
      "blocks.5.shortcut.weight tensor(0.3850, device='cuda:0')\n",
      "----------\n",
      "blocks.6.conv1.weight tensor(-0.4539, device='cuda:0')\n",
      "----------\n",
      "blocks.6.bn1.weight tensor(7.0576e-05, device='cuda:0')\n",
      "----------\n",
      "blocks.6.bn1.bias tensor(0.4302, device='cuda:0')\n",
      "----------\n",
      "blocks.6.conv2.weight tensor(6.7408, device='cuda:0')\n",
      "----------\n",
      "blocks.6.bn2.weight tensor(8.0695, device='cuda:0')\n",
      "----------\n",
      "blocks.6.bn2.bias tensor(7.0222, device='cuda:0')\n",
      "----------\n",
      "blocks.6.shortcut.0.weight tensor(0.0105, device='cuda:0')\n",
      "----------\n",
      "blocks.6.shortcut.1.weight tensor(8.6197, device='cuda:0')\n",
      "----------\n",
      "blocks.6.shortcut.1.bias tensor(7.0222, device='cuda:0')\n",
      "----------\n",
      "blocks.7.conv1.weight tensor(1.2716, device='cuda:0')\n",
      "----------\n",
      "blocks.7.bn1.weight tensor(6.8039e-05, device='cuda:0')\n",
      "----------\n",
      "blocks.7.bn1.bias tensor(0.5140, device='cuda:0')\n",
      "----------\n",
      "blocks.7.conv2.weight tensor(5.4082, device='cuda:0')\n",
      "----------\n",
      "blocks.7.bn2.weight tensor(7.9127, device='cuda:0')\n",
      "----------\n",
      "blocks.7.bn2.bias tensor(6.9063, device='cuda:0')\n",
      "----------\n",
      "blocks.7.shortcut.weight tensor(16.6892, device='cuda:0')\n",
      "----------\n",
      "blocks.8.conv1.weight tensor(-0.9260, device='cuda:0')\n",
      "----------\n",
      "blocks.8.bn1.weight tensor(6.7151e-05, device='cuda:0')\n",
      "----------\n",
      "blocks.8.bn1.bias tensor(0.5042, device='cuda:0')\n",
      "----------\n",
      "blocks.8.conv2.weight tensor(8.8038, device='cuda:0')\n",
      "----------\n",
      "blocks.8.bn2.weight tensor(7.9038, device='cuda:0')\n",
      "----------\n",
      "blocks.8.bn2.bias tensor(6.8322, device='cuda:0')\n",
      "----------\n",
      "blocks.8.shortcut.weight tensor(24.6019, device='cuda:0')\n",
      "----------\n",
      "fc.weight tensor(2093.3330, device='cuda:0')\n",
      "----------\n",
      "fc.bias tensor(10., device='cuda:0')\n",
      "----------\n",
      "Squared copy model grad sum: 2232.683837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results/pruned/lottery/wide-resnet20/synflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77.322560</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045062</td>\n",
       "      <td>1474.282265</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_loss    test_loss  top1_accuracy  top5_accuracy\n",
       "0         NaN    77.322560           10.0          48.49\n",
       "1    0.045062  1474.282265           10.0          50.00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 1\n",
    "args['prune_epochs'] = 1\n",
    "args['model'] = \"wide-resnet20\"\n",
    "args['dataset'] = \"cifar10\"\n",
    "args['lr'] = 0.001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = \"lottery\"\n",
    "args['pruner'] = \"synflow\"\n",
    "args['compression'] = 0.0\n",
    "args['save_result'] = False\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
