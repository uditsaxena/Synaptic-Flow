{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/udit/programs/Synaptic-Flow/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/udit/programs/Synaptic-Flow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "\"dataset\" : \"mnist\", # ['mnist','cifar10','cifar100','tiny-imagenet','imagenet']\n",
    "\"model\" : \"fc\", # ['fc','fc-orth','conv','conv-orth','strconv', ... <take rest from main.py>]\n",
    "\"model_class\" : \"default\", # ['default','lottery','tinyimagenet','imagenet']\n",
    "\"dense_classifier\" : False,\n",
    "\"pretrained\" : False,\n",
    "\"optimizer\" : \"adam\", # ['sgd','momentum','adam','rms']\n",
    "\"train_batch_size\" : 128,\n",
    "\"test_batch_size\" : 128,\n",
    "\"pre_epochs\" : 0, # number of epochs to train before pruning\n",
    "\"post_epochs\" : 20, # number of epochs to train after pruning\n",
    "\"lr\" : 0.001,\n",
    "\"lr_drops\" : [30, 60, 80],\n",
    "\"lr_drop_rate\" : 0.1,\n",
    "\"weight_decay\" : 1e-4 ,\n",
    "\"save\" : False,\n",
    "\"scale\" : 1,\n",
    "\n",
    "# pruning args\n",
    "\"pruner\" : \"synflow\", # ['rand','mag','snip','grasp','synflow']\n",
    "\"compression\" : 1.0, # quotient of prunable non-zero prunable parameters before and after pruning (defaul\"t: 1.0)\n",
    "\"prune_epochs\" : 10, # number of iterations for scoring (defaul\"t: 1)\n",
    "\"compression_schedule\" : \"exponential\", # ['linear','exponential']\n",
    "\"mask_scope\" : \"global\", # ['global','local']\n",
    "\"prune_dataset_ratio\" : 10, # ratio of prune dataset size and number of classes (defaul\"t: 10)'\n",
    "\"prune_batch_size\" : 256, # input batch size for pruning (defaul\"t: 256)\n",
    "\"prune_bias\" : False,\n",
    "\"prune_batchnorm\" : False,\n",
    "\"prune_residual\" : False,\n",
    "\"reinitialize\" : False, # whether to reinitialize weight parameters after pruning (defaul\"t: False)\n",
    "\"pruner_list\" : [], # list of pruning strategies for singleshot (defaul\"t: [])\n",
    "\"prune_epoch_list\" : [], # list of prune epochs for singleshot (defaul\"t: [])\n",
    "\"compression_list\" : [], # list of compression ratio exponents for singleshot/multishot (defaul\"t: [])\n",
    "\"level_list\" : [],\n",
    "\"experiment\" : \"prune-only\", # ['example',\"prune-only\",'singleshot','multishot',\n",
    "    # 'unit-conservation', 'layer-conservation','imp-conservation','schedule-conservation']\n",
    "\"expid\" : \"\",\n",
    "\"result_dir\" : \"Results/data\",\n",
    "\"gpu\" : \"0\",\n",
    "\"workers\" : 4,\n",
    "\"no_cuda\" : False,\n",
    "\"seed\" : 1,\n",
    "\"verbose\" : False,\n",
    "\"save_pruned\" : False,\n",
    "\"save_pruned_path\" : \"Results/pruned\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Utils import load\n",
    "from Utils import generator\n",
    "from Utils import metrics\n",
    "from prune import *\n",
    "from train import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "def prune_only(args):\n",
    "    print(\"Pruning only\")\n",
    "    \n",
    "    ## Random Seed and Device ##\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    device = load.device(args[\"gpu\"])\n",
    "    shuffle = False\n",
    "    ## Data ##\n",
    "    print('Loading {} dataset.'.format(args[\"dataset\"]))\n",
    "    input_shape, num_classes = load.dimension(args[\"dataset\"]) \n",
    "    prune_loader = load.dataloader(args[\"dataset\"], args[\"prune_batch_size\"], True, args[\"workers\"]) #, \n",
    "                                   # args[\"prune_dataset_ratio\"] * num_classes)\n",
    "    \n",
    "    train_loader = load.dataloader(args[\"dataset\"], args[\"train_batch_size\"], False, args[\"workers\"])\n",
    "    test_loader = load.dataloader(args[\"dataset\"], args[\"test_batch_size\"], False, args[\"workers\"])\n",
    "    \n",
    "    \n",
    "    ## Model, Loss, Optimizer ##\n",
    "    print('Creating {}-{} model.'.format(args[\"model_class\"], args[\"model\"]))        \n",
    "\n",
    "    model = load.model(args[\"model\"], args[\"model_class\"])(input_shape, \n",
    "                                                     num_classes, \n",
    "                                                     args[\"dense_classifier\"], \n",
    "                                                     args[\"pretrained\"]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "        \n",
    "    opt_class, opt_kwargs = load.optimizer(args[\"optimizer\"])\n",
    "    optimizer = opt_class(generator.parameters(model), lr=args[\"lr\"], \n",
    "                          weight_decay=args[\"weight_decay\"], **opt_kwargs)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args[\"lr_drops\"], \n",
    "                                                     gamma=args[\"lr_drop_rate\"])\n",
    "\n",
    "    \n",
    "    #print(f\"Kernel: {kernel}\")\n",
    "    \n",
    "    ## Pre-Train ##\n",
    "    #print('Pre-Train for {} epochs.'.format(args.pre_epochs))\n",
    "    pre_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                 test_loader, device, 0, args[\"verbose\"])\n",
    "    \n",
    "    \n",
    "    ## Prune ##\n",
    "    print('Pruning with {} for {} epochs.'.format(args[\"pruner\"], args[\"prune_epochs\"]))\n",
    "    pruner = load.pruner(args[\"pruner\"])(generator.masked_parameters(model, args[\"prune_bias\"], \n",
    "                       args[\"prune_batchnorm\"], args[\"prune_residual\"]))\n",
    "    sparsity = 10**(-float(args[\"compression\"]))\n",
    "    print(\"Sparsity: {}\".format(sparsity))\n",
    "    save_pruned_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "    if (args[\"save_pruned\"]):\n",
    "        print(\"Saving pruned models to: %s\" % (save_pruned_path, ))\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_pruned_path)\n",
    "    prune_loop(model, loss, pruner, prune_loader, device, sparsity, \n",
    "               args[\"compression_schedule\"], args[\"mask_scope\"], args[\"prune_epochs\"], \n",
    "               args[\"reinitialize\"], args[\"save_pruned\"], save_pruned_path)\n",
    "    \n",
    "    prune_result = metrics.summary(model, pruner.scores,\n",
    "                                   metrics.flop(model, input_shape, device),\n",
    "                                   lambda p: generator.prunable(p, args[\"prune_batchnorm\"], \n",
    "                                    args[\"prune_residual\"]))\n",
    "    total_params = int((prune_result['sparsity'] * prune_result['size']).sum())\n",
    "    possible_params = prune_result['size'].sum()\n",
    "    total_flops = int((prune_result['sparsity'] * prune_result['flops']).sum())\n",
    "    possible_flops = prune_result['flops'].sum()\n",
    "    print(\"Parameter Sparsity: {}/{} ({:.4f})\".format(total_params, \n",
    "                                                      possible_params, total_params / possible_params))\n",
    "    print(\"FLOP Sparsity: {}/{} ({:.4f})\".format(total_flops, \n",
    "                                                 possible_flops, total_flops / possible_flops))\n",
    "    \n",
    "    ## Post-Train ##\n",
    "    #print('Post-Training for {} epochs.'.format(args.post_epochs))\n",
    "    post_result = train_eval_loop(model, loss, optimizer, scheduler, train_loader, \n",
    "                                  test_loader, device, args[\"post_epochs\"], args[\"verbose\"])\n",
    "    \n",
    "    print(save_pruned_path)\n",
    "    ## Display Results ##\n",
    "#     frames = [pre_result.head(1), post_result.head(1), post_result.tail(1)]\n",
    "#     train_result = pd.concat(frames, keys=['Init.', 'Post-Prune', \"Final\"])\n",
    "\n",
    "#     print(\"Train results:\\n\", train_result)\n",
    "#     print(\"Prune results:\\n\", prune_result)\n",
    "    if (args[\"save_result\"]):\n",
    "        save_result_path = args[\"save_pruned_path\"] + \"/%s/%s/%s\" % (args[\"model_class\"], \n",
    "                                                                 args[\"model\"], args[\"pruner\"],)\n",
    "        if not os.path.exists(save_pruned_path):\n",
    "            os.makedirs(save_result_path)\n",
    "        post_result.to_csv(save_result_path + \"/%s\" % (args[\"dataset\"] + \"_\" + str(args[\"seed\"]) \n",
    "                                                                        + \"_\" + str(args[\"compression\"]) + \".csv\"))\n",
    "    ## Save Results and Model ##\n",
    "    if args[\"save\"]:\n",
    "        print('Saving results.')\n",
    "        pre_result.to_pickle(\"{}/pre-train.pkl\".format(args[\"result_dir\"]))\n",
    "        post_result.to_pickle(\"{}/post-train.pkl\".format(args[\"result_dir\"]))\n",
    "        prune_result.to_pickle(\"{}/compression.pkl\".format(args[\"result_dir\"]))\n",
    "        torch.save(model.state_dict(),\"{}/model.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(optimizer.state_dict(),\"{}/optimizer.pt\".format(args[\"result_dir\"]))\n",
    "        torch.save(pruner.state_dict(),\"{}/pruner.pt\".format(args[\"result_dir\"]))\n",
    "    \n",
    "    # to change\n",
    "    return post_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar10 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11-bn model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow for 100 epochs.\n",
      "Sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:18<00:04,  4.21it/s]Traceback (most recent call last):\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 930550/9231114 (0.1008)\n",
      "FLOP Sparsity: 41381662/153224202 (0.2701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [02:02<00:00,  4.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.311064</td>\n",
       "      <td>10.15</td>\n",
       "      <td>48.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.570313</td>\n",
       "      <td>1.213881</td>\n",
       "      <td>57.71</td>\n",
       "      <td>96.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.064090</td>\n",
       "      <td>0.869591</td>\n",
       "      <td>69.33</td>\n",
       "      <td>98.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680498</td>\n",
       "      <td>0.632139</td>\n",
       "      <td>78.12</td>\n",
       "      <td>99.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.593127</td>\n",
       "      <td>78.91</td>\n",
       "      <td>99.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.193451</td>\n",
       "      <td>1.273772</td>\n",
       "      <td>61.58</td>\n",
       "      <td>96.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.167934</td>\n",
       "      <td>0.436271</td>\n",
       "      <td>84.38</td>\n",
       "      <td>99.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.129766</td>\n",
       "      <td>0.109078</td>\n",
       "      <td>97.07</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.057542</td>\n",
       "      <td>0.081676</td>\n",
       "      <td>97.90</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026889</td>\n",
       "      <td>0.044886</td>\n",
       "      <td>98.91</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>99.97</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003614</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   2.311064          10.15          48.38\n",
       "1     1.570313   1.213881          57.71          96.03\n",
       "2     1.064090   0.869591          69.33          98.54\n",
       "3     0.680498   0.632139          78.12          99.10\n",
       "4     0.345361   0.593127          78.91          99.28\n",
       "5     0.193451   1.273772          61.58          96.73\n",
       "6     0.167934   0.436271          84.38          99.94\n",
       "7     0.129766   0.109078          97.07         100.00\n",
       "8     0.057542   0.081676          97.90         100.00\n",
       "9     0.026889   0.044886          98.91         100.00\n",
       "10    0.010404   0.006221          99.97         100.00\n",
       "11    0.003614   0.003342         100.00         100.00\n",
       "12    0.001701   0.002341         100.00         100.00\n",
       "13    0.001298   0.002007         100.00         100.00\n",
       "14    0.001110   0.001807         100.00         100.00\n",
       "15    0.000987   0.001669         100.00         100.00\n",
       "16    0.000897   0.001572         100.00         100.00\n",
       "17    0.000829   0.001501         100.00         100.00\n",
       "18    0.000775   0.001447         100.00         100.00\n",
       "19    0.000730   0.001407         100.00         100.00\n",
       "20    0.000691   0.001377         100.00         100.00\n",
       "21    0.000655   0.001350         100.00         100.00\n",
       "22    0.000620   0.001326         100.00         100.00\n",
       "23    0.000586   0.001303         100.00         100.00\n",
       "24    0.000552   0.001286         100.00         100.00\n",
       "25    0.000518   0.001270         100.00         100.00\n",
       "26    0.000484   0.001262         100.00         100.00\n",
       "27    0.000451   0.001257         100.00         100.00\n",
       "28    0.000420   0.001262         100.00         100.00\n",
       "29    0.000394   0.001283         100.00         100.00\n",
       "30    0.000372   0.001346         100.00         100.00"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 30\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11-bn\"\n",
    "args['dataset'] =  'cifar10' # 'tiny-imagenet'\n",
    "args['lr'] = 0.0001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow\"\n",
    "args['compression'] = 1.0\n",
    "args['verbosse'] = True\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar100 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11-bn model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow-dist for 100 epochs.\n",
      "Sparsity: 0.01\n",
      "Feeding image_mean\n",
      "Calculating mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 101045/9277284 (0.0109)\n",
      "FLOP Sparsity: 7096682/153270372 (0.0463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [02:02<00:00,  4.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.605134</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.122572</td>\n",
       "      <td>4.133383</td>\n",
       "      <td>7.90</td>\n",
       "      <td>28.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.627807</td>\n",
       "      <td>3.659594</td>\n",
       "      <td>12.89</td>\n",
       "      <td>38.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.325873</td>\n",
       "      <td>3.256981</td>\n",
       "      <td>20.35</td>\n",
       "      <td>48.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.073351</td>\n",
       "      <td>3.174447</td>\n",
       "      <td>21.60</td>\n",
       "      <td>50.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.851671</td>\n",
       "      <td>3.011569</td>\n",
       "      <td>24.31</td>\n",
       "      <td>53.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.667793</td>\n",
       "      <td>2.935781</td>\n",
       "      <td>26.18</td>\n",
       "      <td>56.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.457366</td>\n",
       "      <td>3.007600</td>\n",
       "      <td>25.97</td>\n",
       "      <td>56.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.280894</td>\n",
       "      <td>2.480884</td>\n",
       "      <td>35.26</td>\n",
       "      <td>67.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.116923</td>\n",
       "      <td>2.480918</td>\n",
       "      <td>34.45</td>\n",
       "      <td>67.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.966676</td>\n",
       "      <td>2.278601</td>\n",
       "      <td>38.53</td>\n",
       "      <td>72.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.806086</td>\n",
       "      <td>2.189503</td>\n",
       "      <td>40.39</td>\n",
       "      <td>73.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.653870</td>\n",
       "      <td>2.175944</td>\n",
       "      <td>41.48</td>\n",
       "      <td>74.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.515062</td>\n",
       "      <td>2.173573</td>\n",
       "      <td>41.80</td>\n",
       "      <td>75.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.364565</td>\n",
       "      <td>1.832407</td>\n",
       "      <td>49.24</td>\n",
       "      <td>81.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.236348</td>\n",
       "      <td>1.705384</td>\n",
       "      <td>52.81</td>\n",
       "      <td>83.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.122899</td>\n",
       "      <td>1.709127</td>\n",
       "      <td>51.64</td>\n",
       "      <td>84.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.994578</td>\n",
       "      <td>1.530802</td>\n",
       "      <td>56.04</td>\n",
       "      <td>86.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.898504</td>\n",
       "      <td>1.415637</td>\n",
       "      <td>59.62</td>\n",
       "      <td>88.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.822735</td>\n",
       "      <td>1.460802</td>\n",
       "      <td>57.65</td>\n",
       "      <td>88.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.747731</td>\n",
       "      <td>1.275635</td>\n",
       "      <td>62.26</td>\n",
       "      <td>90.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.672382</td>\n",
       "      <td>1.274929</td>\n",
       "      <td>62.27</td>\n",
       "      <td>90.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.618825</td>\n",
       "      <td>1.274265</td>\n",
       "      <td>62.79</td>\n",
       "      <td>91.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.571365</td>\n",
       "      <td>1.218266</td>\n",
       "      <td>64.35</td>\n",
       "      <td>92.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.546672</td>\n",
       "      <td>1.084295</td>\n",
       "      <td>67.25</td>\n",
       "      <td>94.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.525665</td>\n",
       "      <td>1.077180</td>\n",
       "      <td>67.32</td>\n",
       "      <td>94.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.489932</td>\n",
       "      <td>0.842158</td>\n",
       "      <td>74.22</td>\n",
       "      <td>96.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.425918</td>\n",
       "      <td>1.000464</td>\n",
       "      <td>69.69</td>\n",
       "      <td>94.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.403638</td>\n",
       "      <td>1.065403</td>\n",
       "      <td>68.06</td>\n",
       "      <td>94.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.397767</td>\n",
       "      <td>1.034864</td>\n",
       "      <td>68.82</td>\n",
       "      <td>94.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.351385</td>\n",
       "      <td>0.979985</td>\n",
       "      <td>70.70</td>\n",
       "      <td>94.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   4.605134           1.02           5.48\n",
       "1     4.122572   4.133383           7.90          28.18\n",
       "2     3.627807   3.659594          12.89          38.51\n",
       "3     3.325873   3.256981          20.35          48.02\n",
       "4     3.073351   3.174447          21.60          50.37\n",
       "5     2.851671   3.011569          24.31          53.32\n",
       "6     2.667793   2.935781          26.18          56.45\n",
       "7     2.457366   3.007600          25.97          56.49\n",
       "8     2.280894   2.480884          35.26          67.60\n",
       "9     2.116923   2.480918          34.45          67.71\n",
       "10    1.966676   2.278601          38.53          72.30\n",
       "11    1.806086   2.189503          40.39          73.61\n",
       "12    1.653870   2.175944          41.48          74.60\n",
       "13    1.515062   2.173573          41.80          75.99\n",
       "14    1.364565   1.832407          49.24          81.61\n",
       "15    1.236348   1.705384          52.81          83.18\n",
       "16    1.122899   1.709127          51.64          84.31\n",
       "17    0.994578   1.530802          56.04          86.61\n",
       "18    0.898504   1.415637          59.62          88.27\n",
       "19    0.822735   1.460802          57.65          88.27\n",
       "20    0.747731   1.275635          62.26          90.89\n",
       "21    0.672382   1.274929          62.27          90.92\n",
       "22    0.618825   1.274265          62.79          91.05\n",
       "23    0.571365   1.218266          64.35          92.18\n",
       "24    0.546672   1.084295          67.25          94.25\n",
       "25    0.525665   1.077180          67.32          94.08\n",
       "26    0.489932   0.842158          74.22          96.31\n",
       "27    0.425918   1.000464          69.69          94.87\n",
       "28    0.403638   1.065403          68.06          94.79\n",
       "29    0.397767   1.034864          68.82          94.51\n",
       "30    0.351385   0.979985          70.70          94.97"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 30\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11-bn\"\n",
    "args['dataset'] =  'cifar100' # 'tiny-imagenet'\n",
    "args['lr'] = 0.001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow-dist\"\n",
    "args['compression'] = 2.0\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar10 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11 model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow-dist for 100 epochs.\n",
      "Sparsity: 0.03162277660168379\n",
      "Feeding image_mean\n",
      "Calculating mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 294415/9225610 (0.0319)\n",
      "FLOP Sparsity: 17962473/152921098 (0.1175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:15<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results/pruned/lottery/vgg11/synflow-dist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.302206</td>\n",
       "      <td>9.93</td>\n",
       "      <td>50.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.164463</td>\n",
       "      <td>2.060572</td>\n",
       "      <td>20.56</td>\n",
       "      <td>78.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.948346</td>\n",
       "      <td>2.043545</td>\n",
       "      <td>25.89</td>\n",
       "      <td>83.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.762894</td>\n",
       "      <td>2.232344</td>\n",
       "      <td>27.01</td>\n",
       "      <td>83.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.644475</td>\n",
       "      <td>1.839692</td>\n",
       "      <td>30.62</td>\n",
       "      <td>89.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.549937</td>\n",
       "      <td>1.845559</td>\n",
       "      <td>33.40</td>\n",
       "      <td>84.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.468996</td>\n",
       "      <td>1.629057</td>\n",
       "      <td>37.55</td>\n",
       "      <td>90.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.378861</td>\n",
       "      <td>1.402449</td>\n",
       "      <td>48.23</td>\n",
       "      <td>93.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.309684</td>\n",
       "      <td>1.389588</td>\n",
       "      <td>49.60</td>\n",
       "      <td>94.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.211154</td>\n",
       "      <td>1.194031</td>\n",
       "      <td>54.51</td>\n",
       "      <td>95.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.157376</td>\n",
       "      <td>1.436114</td>\n",
       "      <td>48.76</td>\n",
       "      <td>91.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.125284</td>\n",
       "      <td>1.176520</td>\n",
       "      <td>57.33</td>\n",
       "      <td>95.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.010668</td>\n",
       "      <td>1.108259</td>\n",
       "      <td>60.10</td>\n",
       "      <td>96.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.965892</td>\n",
       "      <td>0.921195</td>\n",
       "      <td>66.04</td>\n",
       "      <td>97.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.936507</td>\n",
       "      <td>0.874474</td>\n",
       "      <td>67.94</td>\n",
       "      <td>97.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.877613</td>\n",
       "      <td>0.959037</td>\n",
       "      <td>65.30</td>\n",
       "      <td>96.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.822264</td>\n",
       "      <td>0.916201</td>\n",
       "      <td>67.32</td>\n",
       "      <td>98.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.748325</td>\n",
       "      <td>0.739839</td>\n",
       "      <td>72.75</td>\n",
       "      <td>98.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.688747</td>\n",
       "      <td>0.836204</td>\n",
       "      <td>70.13</td>\n",
       "      <td>98.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.688521</td>\n",
       "      <td>0.771880</td>\n",
       "      <td>71.94</td>\n",
       "      <td>98.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.635664</td>\n",
       "      <td>0.687520</td>\n",
       "      <td>75.25</td>\n",
       "      <td>98.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   2.302206           9.93          50.79\n",
       "1     2.164463   2.060572          20.56          78.50\n",
       "2     1.948346   2.043545          25.89          83.74\n",
       "3     1.762894   2.232344          27.01          83.61\n",
       "4     1.644475   1.839692          30.62          89.02\n",
       "5     1.549937   1.845559          33.40          84.54\n",
       "6     1.468996   1.629057          37.55          90.73\n",
       "7     1.378861   1.402449          48.23          93.41\n",
       "8     1.309684   1.389588          49.60          94.30\n",
       "9     1.211154   1.194031          54.51          95.76\n",
       "10    1.157376   1.436114          48.76          91.15\n",
       "11    1.125284   1.176520          57.33          95.37\n",
       "12    1.010668   1.108259          60.10          96.17\n",
       "13    0.965892   0.921195          66.04          97.60\n",
       "14    0.936507   0.874474          67.94          97.85\n",
       "15    0.877613   0.959037          65.30          96.87\n",
       "16    0.822264   0.916201          67.32          98.16\n",
       "17    0.748325   0.739839          72.75          98.75\n",
       "18    0.688747   0.836204          70.13          98.51\n",
       "19    0.688521   0.771880          71.94          98.40\n",
       "20    0.635664   0.687520          75.25          98.91"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 20\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11\"\n",
    "args['dataset'] =  'cifar10'\n",
    "args['lr'] = 0.001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow-dist\"\n",
    "args['compression'] = 1.5\n",
    "args['seed'] = 42\n",
    "args['save_result'] = True\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning only\n",
      "Loading cifar10 dataset.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating lottery-vgg11 model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with synflow for 100 epochs.\n",
      "Sparsity: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sparsity: 925046/9225610 (0.1003)\n",
      "FLOP Sparsity: 41091425/152921098 (0.2687)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [01:15<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results/pruned/lottery/vgg11/synflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.310711</td>\n",
       "      <td>10.17</td>\n",
       "      <td>49.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.313412</td>\n",
       "      <td>2.141395</td>\n",
       "      <td>16.64</td>\n",
       "      <td>74.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.958337</td>\n",
       "      <td>1.972080</td>\n",
       "      <td>23.93</td>\n",
       "      <td>83.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.741563</td>\n",
       "      <td>1.682321</td>\n",
       "      <td>35.64</td>\n",
       "      <td>88.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.618244</td>\n",
       "      <td>1.623462</td>\n",
       "      <td>37.63</td>\n",
       "      <td>90.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.470046</td>\n",
       "      <td>1.419434</td>\n",
       "      <td>46.47</td>\n",
       "      <td>93.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.390034</td>\n",
       "      <td>1.349639</td>\n",
       "      <td>51.23</td>\n",
       "      <td>93.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.296421</td>\n",
       "      <td>1.661358</td>\n",
       "      <td>45.19</td>\n",
       "      <td>87.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.230554</td>\n",
       "      <td>1.307816</td>\n",
       "      <td>52.56</td>\n",
       "      <td>93.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.131306</td>\n",
       "      <td>1.113393</td>\n",
       "      <td>59.70</td>\n",
       "      <td>96.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.051913</td>\n",
       "      <td>1.028262</td>\n",
       "      <td>62.66</td>\n",
       "      <td>96.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.022206</td>\n",
       "      <td>1.171535</td>\n",
       "      <td>58.86</td>\n",
       "      <td>94.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.958739</td>\n",
       "      <td>1.101192</td>\n",
       "      <td>61.44</td>\n",
       "      <td>95.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.882342</td>\n",
       "      <td>0.863689</td>\n",
       "      <td>69.59</td>\n",
       "      <td>97.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.819042</td>\n",
       "      <td>0.758990</td>\n",
       "      <td>73.22</td>\n",
       "      <td>98.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.705776</td>\n",
       "      <td>0.788733</td>\n",
       "      <td>72.84</td>\n",
       "      <td>98.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.671049</td>\n",
       "      <td>0.774505</td>\n",
       "      <td>73.82</td>\n",
       "      <td>98.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.653882</td>\n",
       "      <td>0.763509</td>\n",
       "      <td>74.27</td>\n",
       "      <td>97.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.609216</td>\n",
       "      <td>0.625664</td>\n",
       "      <td>78.01</td>\n",
       "      <td>98.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.513343</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>66.71</td>\n",
       "      <td>97.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.483484</td>\n",
       "      <td>1.023648</td>\n",
       "      <td>67.93</td>\n",
       "      <td>97.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_loss  test_loss  top1_accuracy  top5_accuracy\n",
       "0          NaN   2.310711          10.17          49.45\n",
       "1     2.313412   2.141395          16.64          74.35\n",
       "2     1.958337   1.972080          23.93          83.64\n",
       "3     1.741563   1.682321          35.64          88.46\n",
       "4     1.618244   1.623462          37.63          90.08\n",
       "5     1.470046   1.419434          46.47          93.13\n",
       "6     1.390034   1.349639          51.23          93.69\n",
       "7     1.296421   1.661358          45.19          87.75\n",
       "8     1.230554   1.307816          52.56          93.84\n",
       "9     1.131306   1.113393          59.70          96.34\n",
       "10    1.051913   1.028262          62.66          96.99\n",
       "11    1.022206   1.171535          58.86          94.85\n",
       "12    0.958739   1.101192          61.44          95.59\n",
       "13    0.882342   0.863689          69.59          97.57\n",
       "14    0.819042   0.758990          73.22          98.14\n",
       "15    0.705776   0.788733          72.84          98.37\n",
       "16    0.671049   0.774505          73.82          98.46\n",
       "17    0.653882   0.763509          74.27          97.96\n",
       "18    0.609216   0.625664          78.01          98.84\n",
       "19    0.513343   1.002687          66.71          97.19\n",
       "20    0.483484   1.023648          67.93          97.95"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['post_epochs'] = 20\n",
    "args['prune_epochs'] = 100\n",
    "args['model'] = \"vgg11\"\n",
    "args['dataset'] =  'cifar10'\n",
    "args['lr'] = 0.001\n",
    "args['optimizer'] = 'adam'\n",
    "args['model_class'] = 'lottery'\n",
    "args['pruner'] = \"synflow\"\n",
    "args['compression'] = 1.0\n",
    "args['save_result'] = True\n",
    "prune_only(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG11: \n",
    "- CIFAR 10 - adam, 0.001, 60/120, 0.1, 1e-4 both compression: 1 and 2, 100/160\n",
    "- CIFAR 100 - adam/0.0005, mom/0.001, 60/120, 0.1, 1e-4 both compression: 1 and 2, 100/160\n",
    "- TINYIMAGENET - \n",
    "\n",
    "VGG11-BN:\n",
    "- CIFAR 10 - adam, 0.0001 (comp: 1), 0.0001 (comp: 2), 60/120, 0.1, 1e-4 100/160\n",
    "- CIFAR 100 - adam, 0.0001 (comp: 1), 0.0005 (comp: 2), 60/120, 0.1, 1e-4 100/160\n",
    "\n",
    "VGG16:\n",
    "- CIFAR 10 - adam, 0.001, 60/120, 0.1, 1e-4, compression 1\n",
    "- CIFAR 100 - adam, 0.0001, 60/120, 0.1, 1e-4, compression 1\n",
    "\n",
    "VGG16-BN:\n",
    "- CIFAR 10 - adam, 0.001 1/2 60/120, 0.1, 1e-4\n",
    "- CIFAR 100 - adam, 0.001 (comp: 1)/ 0.0001 (comp: 2), 60/120, 0.1, 1e-4\n",
    "\n",
    "Resnet20:\n",
    "- CIFAR10 - adam 0.0005 (comp: 1), 0.0005 (comp: 2) \n",
    "- CIFAR100 - adam 0.01\n",
    "\n",
    "Resnet18:\n",
    "- Tiny Imagenet:\n",
    "\n",
    "WideResnet20:\n",
    "- CIFAR10 - adam 0.001 (comp: 1/2),\n",
    "- CIFAR100 - adam 0.001 (comp: 1/2), \n",
    "\n",
    "WideResnet18:\n",
    "- Tiny Imagenet: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
