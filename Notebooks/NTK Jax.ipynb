{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: jaxlib in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (0.1.52)\n",
      "Requirement already up-to-date: jax in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (0.1.75)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jaxlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jaxlib) (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jaxlib) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from absl-py->jaxlib) (1.15.0)\n",
      "Collecting neural-tangents\n",
      "  Downloading neural_tangents-0.3.3-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jax>=0.1.75 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from neural-tangents) (0.1.75)\n",
      "Processing /home/udit/.cache/pip/wheels/68/17/69/ac196dd181e620bba5fae5488e4fd6366a7316dce13cf88776/frozendict-1.2-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jax>=0.1.75->neural-tangents) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.12 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jax>=0.1.75->neural-tangents) (1.19.0)\n",
      "Requirement already satisfied: opt-einsum in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from jax>=0.1.75->neural-tangents) (3.3.0)\n",
      "Requirement already satisfied: six in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from absl-py->jax>=0.1.75->neural-tangents) (1.15.0)\n",
      "Installing collected packages: frozendict, neural-tangents\n",
      "Successfully installed frozendict-1.2 neural-tangents-0.3.3\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Using cached tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Processing /home/udit/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.13.0-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow) (0.10.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.31.0-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 100.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=18.1.0 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.23.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 4.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hProcessing /home/udit/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0/future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorflow-datasets) (4.47.0)\n",
      "Collecting requests>=2.19.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.21.1-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (47.3.1.post20200622)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/udit/anaconda3/envs/synflow-py3.7/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: dill, promise\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=404be54950818ab5b8e696e4ca4d422834a543f8785f053ef5b2a77aa8cb0cce\n",
      "  Stored in directory: /home/udit/.cache/pip/wheels/72/6b/d5/5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=f16641c7f0f77036295d69029582ba116f0534ff708094c85a8c09b3bb7b3fbd\n",
      "  Stored in directory: /home/udit/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built dill promise\n",
      "\u001b[31mERROR: tensorflow-metadata 0.23.0 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.10.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, tensorflow-estimator, gast, google-pasta, urllib3, idna, chardet, requests, tensorboard-plugin-wit, werkzeug, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, tensorboard, scipy, termcolor, astunparse, h5py, keras-preprocessing, tensorflow, dill, googleapis-common-protos, tensorflow-metadata, future, promise, tensorflow-datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.0\n",
      "    Uninstalling numpy-1.19.0:\n",
      "      Successfully uninstalled numpy-1.19.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\n",
      "Successfully installed astunparse-1.6.3 cachetools-4.1.1 chardet-3.0.4 dill-0.3.2 future-0.18.2 gast-0.3.3 google-auth-1.21.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 googleapis-common-protos-1.52.0 grpcio-1.31.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 promise-2.3 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-datasets-3.2.1 tensorflow-estimator-2.3.0 tensorflow-metadata-0.23.0 termcolor-1.1.0 urllib3-1.25.10 werkzeug-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jaxlib jax --upgrade \n",
    "!pip install neural-tangents \n",
    "!pip install tensorflow tensorflow-datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import jit, grad, random\n",
    "from jax.experimental import optimizers\n",
    "from neural_tangents import stax\n",
    "from neural_tangents.stax import Dense, Relu\n",
    "from examples import datasets\n",
    "import neural_tangents as nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = datasets.get_dataset(\"mnist\")\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stream():\n",
    "    rng = npr.RandomState(0)\n",
    "\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield train_images[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "batches = data_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "(32, 32)\n",
      "9.133637\n",
      "NTK: Epoch 0 in 2.33 sec\n",
      "Epoch 0 in 5.68 sec\n",
      "(32, 32)\n",
      "9.097219\n",
      "NTK: Epoch 1 in 1.52 sec\n",
      "Epoch 1 in 5.50 sec\n",
      "(32, 32)\n",
      "9.083096\n",
      "NTK: Epoch 2 in 1.53 sec\n",
      "Epoch 2 in 5.52 sec\n",
      "(32, 32)\n",
      "9.060886\n",
      "NTK: Epoch 3 in 1.51 sec\n",
      "Epoch 3 in 5.54 sec\n",
      "(32, 32)\n",
      "9.047582\n",
      "NTK: Epoch 4 in 1.51 sec\n",
      "Epoch 4 in 5.53 sec\n",
      "(32, 32)\n",
      "9.0311\n",
      "NTK: Epoch 5 in 1.50 sec\n",
      "Epoch 5 in 5.54 sec\n",
      "(32, 32)\n",
      "9.01098\n",
      "NTK: Epoch 6 in 1.49 sec\n",
      "Epoch 6 in 5.55 sec\n",
      "(32, 32)\n",
      "8.999\n",
      "NTK: Epoch 7 in 1.51 sec\n",
      "Epoch 7 in 5.54 sec\n",
      "(32, 32)\n",
      "8.979445\n",
      "NTK: Epoch 8 in 1.50 sec\n",
      "Epoch 8 in 5.53 sec\n",
      "(32, 32)\n",
      "8.970364\n",
      "NTK: Epoch 9 in 1.50 sec\n",
      "Epoch 9 in 5.50 sec\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # 10000\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)\n",
    "\n",
    "def loss(params, batch, fn):\n",
    "    inputs, targets = batch\n",
    "    preds = fn(params, inputs)\n",
    "#    print(preds.shape, targets.shape)\n",
    "    return 0.5 * np.mean((preds - targets) ** 2)\n",
    "    # return preds * targets\n",
    "\n",
    "@jit\n",
    "def update(i, opt_state, batch):\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, grad(loss)(params, batch, apply_fn), opt_state)\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "    stax.Dense(1000), stax.Relu(),\n",
    "    stax.Dense(1000), stax.Relu(),\n",
    "    stax.Dense(10)\n",
    ")\n",
    "\n",
    "apply_fn = jit(apply_fn)\n",
    "# kernel_fn = jit(kernel_fn)\n",
    "\n",
    "_, init_params = init_fn(rng, (-1, 28 * 28))\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "itercount = itertools.count()\n",
    "kernel_fn_emp = nt.empirical_kernel_fn(apply_fn, diagonal_axes=()) #trace_axes=(),\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_batches):\n",
    "        opt_state = update(next(itercount), opt_state, next(batches))\n",
    "    epoch_time = time.time() - start_time\n",
    "    perm = npr.RandomState(0).permutation(num_train)\n",
    "    for i in range(1):\n",
    "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "        x_train = train_images[batch_idx]\n",
    "        start_time = time.time()\n",
    "        ntk = kernel_fn_emp\n",
    "        kernel = ntk(x_train, x_train, 'ntk', get_params(opt_state))\n",
    "        # print(kernel)\n",
    "        print(kernel.shape)\n",
    "        print(np.linalg.norm(kernel))\n",
    "        epoch_time_ntk = time.time() - start_time\n",
    "        print(\"NTK: Epoch {} in {:0.2f} sec\".format(epoch, epoch_time_ntk))\n",
    "        # print(np.linalg.norm(kernel_fn_emp(train_images, train_images, 'ntk', get_params(opt_state))))\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8b588c475037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "Starting training...\n",
    "(32, 32, 10, 10)\n",
    "29.026665\n",
    "NTK: Epoch 0 in 2.35 sec\n",
    "Epoch 0 in 5.67 sec\n",
    "(32, 32, 10, 10)\n",
    "28.9128\n",
    "NTK: Epoch 1 in 1.50 sec\n",
    "Epoch 1 in 5.51 sec\n",
    "(32, 32, 10, 10)\n",
    "28.868113\n",
    "NTK: Epoch 2 in 1.53 sec\n",
    "Epoch 2 in 5.51 sec\n",
    "(32, 32, 10, 10)\n",
    "28.797829\n",
    "NTK: Epoch 3 in 1.50 sec\n",
    "Epoch 3 in 5.53 sec\n",
    "(32, 32, 10, 10)\n",
    "28.755636\n",
    "NTK: Epoch 4 in 1.52 sec\n",
    "Epoch 4 in 5.51 sec\n",
    "(32, 32, 10, 10)\n",
    "28.705595\n",
    "NTK: Epoch 5 in 1.52 sec\n",
    "Epoch 5 in 5.65 sec\n",
    "(32, 32, 10, 10)\n",
    "28.639294\n",
    "NTK: Epoch 6 in 1.52 sec\n",
    "Epoch 6 in 5.50 sec\n",
    "(32, 32, 10, 10)\n",
    "28.601112\n",
    "NTK: Epoch 7 in 1.53 sec\n",
    "Epoch 7 in 5.48 sec\n",
    "(32, 32, 10, 10)\n",
    "28.539501\n",
    "NTK: Epoch 8 in 1.52 sec\n",
    "Epoch 8 in 5.50 sec\n",
    "(32, 32, 10, 10)\n",
    "28.510082\n",
    "NTK: Epoch 9 in 1.53 sec\n",
    "Epoch 9 in 5.48 sec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
